---
tags: review
aliases:
cssclass:
---
 
---
tags: review
aliases:
cssclass:
---
 
The basic intuition behind information theory is that learning that an unlikely event has occurred is more informative than learning that a likely event has occurred. A message saying “the sun rose this morning” is so uninformative as to be unnecessary to send, but a message saying “there was a solar eclipse this morning” is very informative.

We would like to quantify information in a way that formalizes this intuition.

1. Likely events should have low information content, and in the extreme case, events that are guaranteed to happen should have no information content whatsoever. 
2. Less likely events should have higher information content. 
3. Independent events should have additive information. For example, finding out that a tossed coin has come up as heads twice should convey twice as much information as finding out that a tossed coin has come up as heads once.

In other words, the Shannon entropy of a distribution is the expected amount of information in an event drawn from that distribution